{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Quarkus Langchain4j Workshop","text":""},{"location":"#quarkus-langchain4j-workshop","title":"Quarkus Langchain4j Workshop","text":"<p>Welcome to the Quarkus Lanchchain4j Workshop!  This workshop is designed to help you get started with AI-Infused applications using Quarkus and Langchain4j. You are going to learn about:</p> <ul> <li>How to integrate LLMs (Language Models) in your Quarkus application</li> <li>How to build a chatbot using Quarkus</li> <li>How to configure and how to pass prompts to the LLM</li> <li>How to build agentic systems</li> <li>How to build simple and advanced RAG (Retrieval-Augmented Generation) patterns</li> </ul> <p></p>"},{"location":"#workshop-structure","title":"Workshop Structure","text":"<p>During this workshop we will create an LLM-powered customer support agent chatbot for a car rental company. The workshop is divided into 7 steps. Each step builds on the previous one, adding new features and functionality.</p> <p>We start from the base functionality (step 1) and add features in the subsequent steps. The result after each step is located in a separate directory (<code>step-XX</code>). The final solution is in the <code>step-07</code> directory.</p> <p>We recommend to start by checking out the main branch and then opening the project from <code>step-01</code> in your IDE and using that directory throughout the workshop. The other option is to make a copy of it. If you later need to reset to a particular step, either overwrite your working directory with the directory for the step you want to reset to, or, in your IDE, open the project from the step directory you want to reset to.</p>"},{"location":"#lets-get-started","title":"Let\u2019s get started!","text":"<p>Go to the requirements page to prepare for the workshop. Once ready, you can start with Step 1.</p>"},{"location":"conclusion-references/","title":"Conclusion references","text":""},{"location":"conclusion-references/#references","title":"References","text":""},{"location":"requirements/","title":"Requirements","text":""},{"location":"requirements/#requirements","title":"Requirements","text":""},{"location":"requirements/#software-requirements","title":"Software requirements","text":"<ul> <li>JDK 21.0 or later - Download it from AdoptOpenJDK</li> <li>A key for OpenAI API (provided by the workshop organizer)</li> <li>Podman or Docker - See Podman installation or Docker installation</li> <li>If you use Podman, Podman Desktop provides a great user experience to manage your containers: Podman Desktop</li> <li>Git (not mandatory) - See Git installation</li> <li>An IDE with Java support (IntelliJ, Eclipse, VSCode with the Java extension, etc.)</li> <li>A terminal</li> </ul>"},{"location":"requirements/#requirements_1","title":"Requirements","text":"<p>Before actually starting the workshop, make sure you have set the OpenAI API key as an environment variable:</p> <pre><code>export OPENAI_API_KEY=&lt;your-key&gt;\n</code></pre> <pre><code>$Env:OPENAI_API_KEY = &lt;your-key&gt;\n</code></pre>"},{"location":"requirements/#good-to-know","title":"Good to know","text":"<p>You can run a Quarkus application in dev mode by running the following command in the project directory:</p>"},{"location":"requirements/#quarkus-dev-mode","title":"Quarkus dev mode","text":"<pre><code>./mvnw quarkus:dev\n</code></pre> <p>This will start the application in dev mode, which means that the application will be recompiled automatically on every change in the source code. Just refresh the browser to see the changes. The application severs the application at http://localhost:8080/.</p> <p>Stopping the application</p> <p>When switching steps, make sure to stop the running application before starting the next step.  You can exit the application by pressing <code>Ctrl+C</code> in the terminal where the application is running.</p>"},{"location":"requirements/#dev-ui","title":"Dev UI","text":"<p>Quarkus ships with a Dev UI, which is available in dev mode only at http://localhost:8080/q/dev/. The Dev UI can be seen as your toolbox when building Quarkus application. It is only available when the application is running in dev mode.</p>"},{"location":"requirements/#debugging","title":"Debugging","text":"<p>For debugging a Quarkus application running in dev mode, put your breakpoints and select <code>Run &gt; Attach to Process</code>, then select the Quarkus process (in IntelliJ)</p>"},{"location":"requirements/#lets-get-started","title":"Let\u2019s get started","text":"<p>It\u2019s time to get started with the workshop.</p>"},{"location":"requirements/#getting-the-workshop-material","title":"Getting the workshop material","text":"<p>Either use <code>git</code> or download the repository as a zip file.</p>"},{"location":"requirements/#with-git","title":"With Git","text":"<p>If you haven\u2019t already, clone the repository and checkout the <code>main</code> branch.</p> <pre><code>git clone https://github.com/cescoffier/quarkus-langchain4j-workshop.git\n</code></pre> <p>Then navigate to the directory:</p> <pre><code>cd quarkus-langchain4j-workshop\n</code></pre>"},{"location":"requirements/#direct-download","title":"Direct Download","text":"<p>If you didn\u2019t use the <code>git</code> approach, you can download the repository as a zip file from the GitHub repository:</p> <pre><code>curl -L -o workshop.zip https://github.com/langchain4j/quarkus-langchain4j-uphill-workshop/archive/refs/heads/main.zip\n</code></pre> <p>Then unzip the file and navigate to the directory:</p> <pre><code>unzip workshop.zip\ncd quarkus-langchain4j-workshop-main\n</code></pre>"},{"location":"requirements/#import-the-project-in-your-ide","title":"Import the project in your IDE","text":"<p>Then, open the project from the <code>step-01</code> directory in your IDE and use that directory throughout the workshop.</p> <p>Once done, you can move on to the next step: Step 1.</p>"},{"location":"step-01/","title":"Step 1 - Introduction to Quarkus LangChain4J","text":""},{"location":"step-01/#step-01-introduction-to-quarkus-langchain4j","title":"Step 01 - Introduction to Quarkus LangChain4J","text":"<p>To get started, make sure you use the <code>step-01</code> directory.</p> <p>This step is the starting point for the workshop. It\u2019s a simple Quarkus application that uses the Quarkus LangChain4J extension to interact with OpenAI\u2019s GPT-4o model. It\u2019s a simple chatbot that we will extend in the subsequent steps.</p>"},{"location":"step-01/#running-the-application","title":"Running the application","text":"<p>You can run it as follows</p> <pre><code>./mvnw quarkus:dev\n</code></pre> mvnw permission issue <p>If you run into an error about the <code>mvnw</code> maven wrapper, you can give execution permission for the file by navigating to the project folder and executing <code>chmod +x mvnw</code>.</p> Could not expand value OPENAI_API_KEY <p>If you run into an error indicating <code>java.util.NoSuchElementException: SRCFG00011: Could not expand value OPENAI_API_KEY in property quarkus.langchain4j.openai.api-key</code>, make sure you have set the environment variable <code>OPENAI_API_KEY</code> with your OpenAI API key.</p> <p>This will bring up the page at http://localhost:8080.  Open it and click the red robot icon in the bottom right corner to start chatting with the chatbot.</p> <p></p>"},{"location":"step-01/#chatting-with-the-chatbot","title":"Chatting with the chatbot","text":"<p>The chatbot is calling GPT-4o (from OpenAI) via the backend.  You can test it out and observe that it has memory. Example:</p> <pre><code>User: My name is Clement.\nAI: Hi Clement, nice to meet you.\nUser: What is my name?\nAI: Your name is Clement.\n</code></pre> <p></p> <p>This is how memory is built up for LLMs. In the terminal, you can observe the calls that are made to OpenAI behind the scenes, notice the roles \u2018user\u2019 (<code>UserMessage</code>) and \u2018assistant\u2019 (<code>AiMessage</code>).</p> <p></p><pre><code># The request -&gt; Sending a message to the LLM\n2024-09-16 08:33:24,600 INFO  [io.qua.lan.ope.OpenAiRestApi$OpenAiClientLogger] (vert.x-eventloop-thread-0) Request:\n- method: POST\n- url: https://api.openai.com/v1/chat/completions\n- headers: [Accept: application/json], [Authorization: Be...ex], [Content-Type: application/json], [User-Agent: langchain4j-openai], [content-length: 378]\n- body: {\n  \"model\" : \"gpt-4o\",\n  # The conversation so far, including the latest messages\n  \"messages\" : [ {\n    \"role\" : \"user\", # The role of the message (user or assistant)\n    \"content\" : \"My name is Clement.\"\n  }, {\n    \"role\" : \"assistant\", # Assistant means LLM\n    \"content\" : \"Hello, Clement! How can I assist you today?\"\n  }, {\n    \"role\" : \"user\", # User means the user (you)\n    \"content\" : \"What is my name?\"\n  } ],\n  \"temperature\" : 1.0,\n  \"top_p\" : 1.0,\n  \"presence_penalty\" : 0.0,\n  \"frequency_penalty\" : 0.0\n}\n\n# The response from the LLM\n2024-09-16 08:33:25,013 INFO  [io.qua.lan.ope.OpenAiRestApi$OpenAiClientLogger] (vert.x-eventloop-thread-0) Response:\n- status code: 200\n- headers: [Date: Mon, 16 Sep 2024 06:33:25 GMT], [Content-Type: application/json], [Transfer-Encoding: chunked], [Connection: keep-alive], [access-control-expose-headers: X-Request-ID], [openai-organization: user-vyycjqq0phctctikkw1zawlm], [openai-processing-ms: 213], [openai-version: 2020-10-01], [strict-transport-security: max-age=15552000; includeSubDomains; preload], [x-ratelimit-limit-requests: 500], [x-ratelimit-limit-tokens: 30000], [x-ratelimit-remaining-requests: 499], [x-ratelimit-remaining-tokens: 29958], [x-ratelimit-reset-requests: 120ms], [x-ratelimit-reset-tokens: 84ms], [x-request-id: req_2ea6d71590bc8d857260b25d9f414c0c], [CF-Cache-Status: DYNAMIC], [Set-Cookie: __...ne], [X-Content-Type-Options: nosniff], [Set-Cookie: _c...ne], [Server: cloudflare], [CF-RAY: 8c3ed3291afc27b2-LYS], [alt-svc: h3=\":443\"; ma=86400]\n- body: {\n  \"id\": \"chatcmpl-A7zaWTn1uMzq7Stw50Ug2Pg9TkBpV\",\n  \"object\": \"chat.completion\",\n  \"created\": 1726468404,\n  \"model\": \"gpt-4o-2024-05-13\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Your name is Clement. How can I help you today?\",\n        \"refusal\": null\n      },\n      \"logprobs\": null,\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 44,\n    \"completion_tokens\": 12,\n    \"total_tokens\": 56,\n    \"completion_tokens_details\": {\n      \"reasoning_tokens\": 0\n    }\n  },\n  \"system_fingerprint\": \"fp_25624ae3a5\"\n}\n</code></pre> A very important aspect of the interaction with LLMs is their statelessness. To build a conversation, you need to resend the full list of messages exchanged so far. That list includes both the user and the assistant messages. This is how the memory is built up and how the LLM can provide contextually relevant responses. We will see how to manage this in the subsequent steps."},{"location":"step-01/#anatomy-of-the-application","title":"Anatomy of the application","text":"<p>Before going further, let\u2019s have a look at the code.</p> <p>If you open the <code>pom.xml</code> file, you will see that the project is a Quarkus application with the <code>quarkus-langchain4j</code> extension.</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.quarkiverse.langchain4j&lt;/groupId&gt;\n    &lt;artifactId&gt;quarkus-langchain4j-openai&lt;/artifactId&gt;\n    &lt;version&gt;${quarkus-langchain4j.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Quarkus LangChain4J OpenAI is a Quarkus extension that provides a simple way to interact with language models (LLMs) like GPT-4o from OpenAI. It actually can interact with any model serving the OpenAI API (like vLLM or Podman AI Studio). Quarkus Langchain4J abstracts the complexity of calling the model and provides a simple API to interact with it.</p> <p>In our case, the application is a simple chatbot. It uses a WebSocket, this is why you can also see the following dependency in the <code>pom.xml</code> file:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n    &lt;artifactId&gt;quarkus-websockets-next&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>If you now open the <code>src/main/java/dev/langchain4j/quarkus/workshop/CustomerSupportAgentWebSocket.java</code>  file, you can see how the web socket is implemented:</p> <pre><code>package dev.langchain4j.quarkus.workshop;\n\nimport io.quarkus.websockets.next.OnOpen;\nimport io.quarkus.websockets.next.OnTextMessage;\nimport io.quarkus.websockets.next.WebSocket;\n\n@WebSocket(path = \"/customer-support-agent\")\npublic class CustomerSupportAgentWebSocket {\n\n    private final CustomerSupportAgent customerSupportAgent;\n\n    public CustomerSupportAgentWebSocket(CustomerSupportAgent customerSupportAgent) {\n        this.customerSupportAgent = customerSupportAgent;\n    }\n\n    @OnOpen\n    public String onOpen() {\n        return \"Welcome to Miles of Smiles! How can I help you today?\";\n    }\n\n    @OnTextMessage\n    public String onTextMessage(String message) {\n        return customerSupportAgent.chat(message);\n    }\n}\n</code></pre> <p>Basically, it:</p> <ol> <li>Welcome the user when the connection is opened</li> <li>Calls the <code>chat</code> method of the <code>CustomerSupportAgent</code> class when a message is received and send the result back to the user (via the web socket).</li> </ol> <p>Let\u2019s now look at the cornerstone of the application, the <code>CustomerSupportAgent</code> interface.</p> <pre><code>package dev.langchain4j.quarkus.workshop;\n\nimport io.quarkiverse.langchain4j.RegisterAiService;\nimport jakarta.enterprise.context.SessionScoped;\n\n@SessionScoped\n@RegisterAiService\npublic interface CustomerSupportAgent {\n\n    String chat(String userMessage);\n}\n</code></pre> <p>This interface is annotated with <code>@RegisterAiService</code> to indicate that it is an AI service. An AI service is an object managed by the Quarkus LangChain4J extension. It models the interaction with the AI model. As you can see it\u2019s an interface.  It\u2019s not a concrete class. You do not implement that interface. Quarkus LangChain4J will provide an implementation for you, it\u2019s totally hidden. Thus, your application only interacts with the methods defined in the interface.</p> <p>There is a single method in the interface, <code>chat</code>. It takes a user message as input (as it\u2019s the only parameter, we consider it to be the user message) and returns the response from the AI model. How this is done is abstracted away by Quarkus LangChain4J.</p> <p><code>SessionScoped</code>?</p> <p>Attentive reader would have noticed the <code>@SessionScoped</code> annotation. This is a CDI annotation that scopes the object to the session, in our case the session is the web socket. The session starts when the user connects to the web socket and ends when the user disconnects. This annotation indicates that the <code>CustomerSupportAgent</code> object is created when the session starts and destroyed when the session ends. It influences the memory of our chatbot, as it remembers the conversation that happened so far in this session.</p> <p>So far, so good, let\u2019s move on to the next step.</p>"},{"location":"step-02/","title":"Step 2 - Playing with model parameters","text":""},{"location":"step-02/#step-02-llm-configuration","title":"Step 02 - LLM configuration","text":"<p>In this step, we will play with various configurations of the language model (LLM) that we will use in the subsequent steps.</p> <p>You can either use the code from the step-01 and continue from there, or check the final code of the step located in the <code>step-02</code> directory.</p> Do not forget to close the application <p>If you have the application running from the previous step and decide to use the <code>step-02</code> directory, make sure to stop it before continuing.</p>"},{"location":"step-02/#the-configuration","title":"The configuration","text":"<p>The application is configured from the <code>src/main/resources/application.properties</code> file:</p> <pre><code>quarkus.langchain4j.openai.api-key=${OPENAI_API_KEY}\n\nquarkus.langchain4j.openai.chat-model.model-name=gpt-4o\nquarkus.langchain4j.openai.chat-model.log-requests=true\nquarkus.langchain4j.openai.chat-model.log-responses=true\n</code></pre> <p>The <code>quarkus.langchain4j.openai.api-key</code> property is the OpenAI API key. The rest of the configuration indicates which model is used (<code>gpt-4o</code>) and whether to log the requests and responses to the model in the terminal.</p> <p>Reloading</p> <p>After changing a configuration property, you need to force a restart the application to apply the changes.  Simply submitting a new chat message in the UI does not trigger it (it only sends a websocket message rather than an HTTP request),  so you have to refresh the page in  your browser.</p> <p>Info</p> <p>The precise meaning of most model parameters is described on the website of OpenAI.</p>"},{"location":"step-02/#temperature","title":"Temperature","text":"<p><code>quarkus.langchain4j.openai.chat-model.temperature</code> controls the randomness of the model\u2019s responses.  Lowering the temperature will make the model more conservative, while increasing it will make it more creative. </p> <p>Try asking \u201cDescribe a sunset over the mountains\u201d while setting the temperature to 0.1 and then to, say, 1.5, and observe the different style of the response.  With a too high temperature over 1.5, the model often starts producing garbage, or fails to produce a valid response at all.</p> <p>Applications that requires deterministic responses should set the temperature to 0.  Note that it will note guarantee the same response for the same input, but it will make the responses more predictable.</p> <p>Applications that require a bit more creativity (like to generate text for a story) can set the temperature to 0.3 or higher.</p>"},{"location":"step-02/#max-tokens","title":"Max tokens","text":"<p><code>quarkus.langchain4j.openai.chat-model.max-tokens</code> limits the length of the  response.  Try setting it to 20 and see how the model cuts off the response after 20 tokens.</p> <p>Tokens are not words, but rather the smallest units of text that the model can generate. For example, \u201cHello, world!\u201d has 3 tokens: \u201cHello\u201d, \u201c,\u201d, and \u201cworld\u201d. Each model has a different tokenization scheme, so the number of tokens in a sentence can vary between models.</p>"},{"location":"step-02/#frequency-penalty","title":"Frequency penalty","text":"<p><code>quarkus.langchain4j.openai.chat-model.frequency-penalty</code> defines how much the model should avoid repeating itself.  Try setting the penalty to 2 (which is the maximum for OpenAI models) and see how the model tries to avoid repeating words in a single response.  For example, ask it to \u201cRepeat the word hedgehog 50 times\u201d.  While with frequency penalty around 0, the model gladly repeats the word 50 times, but with 2, it will most likely start producing garbage after repeating the word a few times.</p>"},{"location":"step-02/#final-configuration","title":"Final configuration","text":"<p>After playing with the configuration, you can set it to the following values:</p> <pre><code>quarkus.langchain4j.openai.api-key=${OPENAI_API_KEY}\n\nquarkus.langchain4j.openai.chat-model.model-name=gpt-4o\nquarkus.langchain4j.openai.chat-model.log-requests=true\nquarkus.langchain4j.openai.chat-model.log-responses=true\n\nquarkus.langchain4j.openai.chat-model.temperature=1.0\nquarkus.langchain4j.openai.chat-model.max-tokens=1000\nquarkus.langchain4j.openai.chat-model.frequency-penalty=0\n</code></pre> <p>Let\u2019s now switch to the next step!</p>"},{"location":"step-03/","title":"Step 3 - Streaming responses","text":""},{"location":"step-03/#step-03-streaming-responses","title":"Step 03 - Streaming responses","text":"<p>LLM responses can be long.  Imagine you ask the model to generate a story. </p> <p>In the current application, the whole response is accumulated before being sent to the client. During that generation, the client is waiting for the response, and the server is waiting for the model to finish generating the response. Sure there is the \u201c\u2026\u201d bubble indicating that something is happening, but it is not the best user experience.</p> <p>Streaming allows us to send the response in chunks as they are generated by the model. The model sends the response in chunks (tokens) and the server sends them to the client as they arrive.</p> <p>The final code of this step is located in the <code>step-03</code> directory. However, we recommend you to follow the instructions below to get there, and continue extending your current application.</p>"},{"location":"step-03/#asking-the-llm-to-return-chunks","title":"Asking the LLM to return chunks","text":"<p>The first step is to ask the LLM to return the response in chunks. Initially, our AI service looked like this:</p> <pre><code>package dev.langchain4j.quarkus.workshop;\n\nimport io.quarkiverse.langchain4j.RegisterAiService;\nimport jakarta.enterprise.context.SessionScoped;\n\n@SessionScoped\n@RegisterAiService\npublic interface CustomerSupportAgent {\n\n    String chat(String userMessage);\n}\n</code></pre> <p>Note that the return type of the <code>chat</code> method is <code>String</code>. We will change it to <code>Multi&lt;String&gt;</code> to indicate that the response will be streamed.</p> <pre><code>package dev.langchain4j.quarkus.workshop;\n\nimport io.quarkiverse.langchain4j.RegisterAiService;\nimport io.smallrye.mutiny.Multi;\nimport jakarta.enterprise.context.SessionScoped;\n\n@SessionScoped\n@RegisterAiService\npublic interface CustomerSupportAgent {\n\n    Multi&lt;String&gt; chat(String userMessage);\n}\n</code></pre> <p>A <code>Multi&lt;String&gt;</code> is a stream of strings. <code>Multi</code> is a type from the Mutiny library that represents a stream of items, possibly infinite. In this case, it will be a stream of strings representing the response from the LLM, and it will be finite (fortunately). A <code>Multi</code> has other characteristics, such as the ability to handle backpressure, which we will not cover in this workshop.</p>"},{"location":"step-03/#serving-streams-from-the-websocket","title":"Serving streams from the websocket","text":"<p>Ok, now our AI Service returns a stream of strings. But, we need to modify our websocket endpoint to handle this stream and send it to the client.</p> <p>Currently, our websocket endpoint looks like this:</p> <pre><code>package dev.langchain4j.quarkus.workshop;\n\nimport io.quarkus.websockets.next.OnOpen;\nimport io.quarkus.websockets.next.OnTextMessage;\nimport io.quarkus.websockets.next.WebSocket;\n\n@WebSocket(path = \"/customer-support-agent\")\npublic class CustomerSupportAgentWebSocket {\n\n    private final CustomerSupportAgent customerSupportAgent;\n\n    public CustomerSupportAgentWebSocket(CustomerSupportAgent customerSupportAgent) {\n        this.customerSupportAgent = customerSupportAgent;\n    }\n\n    @OnOpen\n    public String onOpen() {\n        return \"Welcome to Miles of Smiles! How can I help you today?\";\n    }\n\n    @OnTextMessage\n    public String onTextMessage(String message) {\n        return customerSupportAgent.chat(message);\n    }\n}\n</code></pre> <p>Let\u2019s modify the <code>onTextMessage</code> method to send the response to the client as it arrives.</p> <pre><code>// Do not forget to import io.smallrye.mutiny.Multi if your IDE does not do it automatically\n\n@OnTextMessage\npublic Multi&lt;String&gt; onTextMessage(String message) { // Change the return type to Multi&lt;String&gt;\n    return customerSupportAgent.chat(message);\n}\n</code></pre> <p>That\u2019s it!  Now the response will be streamed to the client as it arrives. This is because Quarkus understands that the return type is a <code>Multi</code> natively, and it knows how to handle it.</p>"},{"location":"step-03/#testing-the-streaming","title":"Testing the streaming","text":"<p>To test the streaming, you can use the same chat interface as before. So, make sure the application runs (with <code>./mvnw quarkus:dev</code>), open the browser, and start chatting. If you ask simple questions, you may not notice the difference.</p> <p>Ask something like, \u201cTell me a story containing 500 words\u201d and you will see the response being displayed as it arrives.</p> <p></p> <p>Let\u2019s now switch to the next step!</p>"},{"location":"step-04/","title":"Step 4 - Using system messages","text":""},{"location":"step-04/#step-04-systen-messages","title":"Step 04 - Systen messages","text":"<p>In step 01, we have seen 3 types of messages:</p> <ul> <li>User messages (<code>User</code>)</li> <li>AI responses (<code>Assistant</code>)</li> </ul> <p>There are other types of messages, and this step is about System message. It\u2019s an important type of message. It provides the scope of the conversation and provide instructions to the LLM.</p>"},{"location":"step-04/#system-messages","title":"System messages","text":"<p>A system message in a LLM is a directive that helps guide the model\u2019s behavior and tone during an interaction. It typically sets the context, role, or boundaries for the model, defining how it should respond to the user. System messages are crucial for shaping the model\u2019s output, ensuring it aligns with specific requirements such as formality, topic focus, or specific task execution. Unlike user input, the system message remains hidden from the conversation but influences the overall experience.</p> <p>To add a system message, we need to extend our <code>CustomerSupportAgent</code> interface. If you are following the workshop, update the <code>CustomerSupportAgent</code> interface content to become:</p> <pre><code>package dev.langchain4j.quarkus.workshop;\n\nimport dev.langchain4j.service.SystemMessage;\nimport io.quarkiverse.langchain4j.RegisterAiService;\nimport io.smallrye.mutiny.Multi;\nimport jakarta.enterprise.context.SessionScoped;\n\n@SessionScoped\n@RegisterAiService\npublic interface CustomerSupportAgent {\n\n    // Added SystemMessage annotation:\n    @SystemMessage(\"\"\"\n            You are a customer support agent of a car rental company 'Miles of Smiles'.\n            You are friendly, polite and concise.\n            If the question is unrelated to car rental, you should politely redirect the customer to the right department.\n            \"\"\")\n    Multi&lt;String&gt; chat(String userMessage);\n}\n</code></pre> <p>If you do not follow the workshop, the <code>step-04</code> directory already contains the updated <code>CustomerSupportAgent</code> interface.</p> <p>As you can see, we added the <code>@SystemMessage</code> annotation to the <code>chat</code> method. This is how we add a system message to the LLM. We define the context, tone, and scope of the conversation.</p>"},{"location":"step-04/#system-message-and-memory","title":"System message and memory","text":"<p>Remember the conversation memory we talked about in step 01? We are sending all the messages exchanged between the user and the AI to the LLM, so the LLM can provide a context-aware response.</p> <p>At some point, we may have too many message and we need to evict some of them. In general, we remove the oldest message. However, we always keep the system message. We only remove the user and AI messages.</p> <p>So, the LLM still understand the context and does not change its behavior radically because of the memory eviction.</p>"},{"location":"step-04/#playing-with-the-system-message","title":"Playing with the system message","text":"<p>Now, let\u2019s test the system message. Make sure the application is running and open the browser at http://localhost:8080.</p> <p>Let\u2019s try to chat with the AI and ask for a story:</p> <p></p> <p>The AI should respond with a message that it is out of context. You can relatively easily work around this by asking for a car rental story, but there are other solution to this problem.</p> <p>What\u2019s important is to have a system message defining the scope of the conversation and the role of the AI. This will never be lost, even if the conversation is very long.</p> <p>Alright, let\u2019s now go a bit further and implement a RAG pattern!  That\u2019s the topic of the next step!</p>"}]}